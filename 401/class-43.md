# Class 43 Reading Notes

## Ethics in the workplace

### Google Backtracks, Says Its AI Will Not Be Used for Weapons or Surveillance

Google employees protested the use of artificial intelligence in a drone survailance program with the DOD. Google released a new set of AI principles, in which they claim will follow their personal beliefs better. They want to make artificial intelligence that does no harm to people, and is only socially beneficial.

I'm a little torn on this subject. I feel that there was no need for anyone to change their stance on the subject. The article itself talks about the how the team involved in helping the DOD utilize the (open source) code, felt that they were doing no harm. The artificial intelligence was only used to analyze video recordings. At what point is that considered harmful? Is all AI that analyzes video wrong? Or did people overreact to a project simpy because it had the military involved? I think that there does need to be a distinction in AI being used for weapons. It shouldn't ever get to the point where AI is making decisions that can harm people, but this clearly wasn't it. 

### The cybersecurity risk of self driving cars

I love this concept! Self driving cars are totally the future. It's incredible to me how much has gone into making even driving assist cars work. I think it is an important topic though. How much computation power is being done outside of the car itself? At the moment, it's all proprietary and we don't know. But, say the car is actually connecting to a server outside somewhere. That means that it is sending packets across the internet that can be intercepted. It is only a matter of time before they are intercepted, sniffed, and altered. If companies aren't incredibly careful, lives could be at stake